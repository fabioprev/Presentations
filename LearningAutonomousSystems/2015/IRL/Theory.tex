\section{Theory}

\begin{frame}
	\frametitle{Theoretical Concepts}
	\framesubtitle{Markov Decision Process}
	
	\large
	
	\vspace{0.1cm}
	
	A (finite) discrete-time MDP is a tuple $ (\mathbf{S},\mathbf{A},\mathbf{P}_{sa},
	\mathbf{R},\gamma) $ where:
	\vspace{0.1cm}
	\begin{itemize}
		\item $ \mathbf{S} $ is a finite set of N states
		\vspace{0.1cm}
		\item $ \mathbf{A} = \{ a_1, a_2, \ldots, a_k \} $ is a set of $ k $ actions
		\vspace{0.1cm}
		\item $ \mathbf{P}_{sa}(\cdot) $ are the state transition probabilities upon taking
			  action $ a $ in a state $ s $
		\vspace{0.1cm}
		\item $ \mathbf{R} : \mathbf{S} \times \mathbf{A}  \mapsto \mathds{R} $ is the
			  reinforcement function, bounded in absolute value by $ R_{max} $
		\vspace{0.1cm}
		\item $ \gamma \in [0,1) $ is the discount factor
		\vspace{0.1cm}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Theoretical Concepts}
	\framesubtitle{Value Function}
	
	\Large
	
	\vspace{0.2cm}
	
	A \textbf{policy} is defined as any map $ \pi : \mathbf{S} \mapsto \mathbf{A} $ and the
	\textbf{value function} for a policy $ \pi $, evaluated at any state $ s_1 $ is given by\\
	
	\vspace{-0.5cm}
	
	\begin{equation*}
		V^\pi(s_1) = E \, \Big [ \, R(s_1) + \gamma R(s_2) + \gamma^2 R(s_3) + \cdots \; | \;
		\pi \, \Big ]
	\end{equation*}
	
	\setstretch{1}
	where the expectation is over the distribution of the state sequence ($ s_1, s_2, \ldots,
	s_N $) we pass through, when we execute the policy $ \pi $ starting from $ s_1 $\\
\end{frame}

\begin{frame}
	\frametitle{Theoretical Concepts}
	\framesubtitle{Q-function}
	
	\Large
	
	The \textbf{Q-function} is defined as
	
	\begin{equation*}
		Q^\pi(s,a) = \mathbf{R}(s) + \gamma E_{s^\prime \sim \mathbf{P}_{sa}(\cdot)} \big [
		V^\pi(s^\prime) \big ]
	\end{equation*}
	
	\vspace{0.5cm}
	
	\setstretch{1}
	where the notation $ s^\prime \sim \mathbf{P}_{sa}(\cdot) $ means the expectation is with
	respect to $ s^\prime $ distributed according to $ \mathbf{P}_{sa}(\cdot) $.\\
\end{frame}

\begin{frame}
	\frametitle{Theoretical Concepts}
	\framesubtitle{Bellman Equations}
	
	\Large
	
	\vspace{0.3cm}
	
	Let a MDP $ \mathcal{M} = (\mathbf{S},\mathbf{A},\mathbf{P}_{sa},\mathbf{R},\gamma) $ and a
	policy $ \pi : \mathbf{S} \mapsto \mathbf{A} $ be given. Then, for all $ s \in \mathbf{S} $,
	$ a \in \mathbf{A} $, $ V^\pi $ and $ Q^\pi $ satisfy
	
	\vspace{-0.2cm}
	
	\begin{eqnarray*}
		V^\pi(s) &=& \mathbf{R}(s) + \gamma \sum_{s^\prime} \mathbf{P}_{s\pi(s)}(s^\prime)V^\pi(s^\prime)\\
		Q^\pi(s,a) &=& \mathbf{R}(s) + \gamma \sum_{s^\prime} \mathbf{P}_{sa}(s^\prime)V^\pi(s^\prime)
	\end{eqnarray*}
\end{frame}

\begin{frame}
	\frametitle{Theoretical Concepts}
	\framesubtitle{Bellman Optimality}
	
	\Large
	
	\setstretch{1}
	Let a MDP $ \mathcal{M} = (\mathbf{S},\mathbf{A},\mathbf{P}_{sa},\mathbf{R},\gamma) $ and a
	policy $ \pi : \mathbf{S} \mapsto \mathbf{A} $ be given. Then $ \pi $ is an \textbf{optimal
	policy} for $ \mathcal{M} $ if and only if, for all $ s \in \mathbf{S} $\\
	
	\begin{equation*}
		\pi(s) \in \arg \max_{a \, \in \, \mathbf{A}} Q^\pi(s,a)
	\end{equation*}
\end{frame}

\begin{frame}
	\frametitle{Theoretical Concepts}
	\framesubtitle{Policy Optimality}
	
	\Large
	
	\setstretch{1}
	Let a finite state space $ \mathbf{S} $, a set of transition probabilities matrices $ \{
	\mathbf{P}_{a} \} $, a set of actions $ \mathbf{A} = \{a_1,\ldots,a_k\} $ and a discount
	factor $ \gamma \in [0,1) $ be given. Then, the policy $ \pi $, given by $ \pi(s) \equiv a_*
	$, is \textbf{optimal} if and only if, for all $ a \in \mathbf{A} $, the reward $ \mathbf{R}
	$ satisfies
	
	\begin{equation*}
		(\mathbf{P}_{a_*} - \mathbf{P}_a)(\mathbf{I} - \gamma \mathbf{P}_{a_*})^{-1} \,
		\mathbf{R} \succeq 0
	\end{equation*}
\end{frame}
