\section{Methods}

\begin{frame}
	\frametitle{t-Distributed Stochastic Neighbor Embedding}
	
	The \emph{t-SNE} technique:
	
	\begin{itemize}
		\item performs a nonlinear dimensionality reduction
		\item models each high-dimensional object by a 2-D or 3-D point in such a way that
			  \begin{itemize}
			  	  \item similar objects are modeled by nearby points
			  	  \item and dissimilar objects are modeled by distant points
			  \end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{t-Distributed Stochastic Neighbor Embedding}
	
	\vspace{0.2cm}
	
	The \emph{t-SNE} algorithm comprises two main stages:
	
	\begin{itemize}
		\item First, it constructs a probability distribution $ P $ over pairs of high-dimensional objects in such a
			  way that similar objects have a high probability of being picked, while dissimilar points have an
			  infinitesimal probability of being picked
	\end{itemize}
	
	\begin{equation*}
		p_{ij} = \frac{p_{j|i} + p_{i|j}}{2N}
	\end{equation*}
	
	\begin{equation*}
		p_{j|i} = \frac{exp(-\| \boldsymbol{x}_i,\boldsymbol{x}_j \|^2 \; / \; 2\sigma_i^2)}{\sum_{k \neq i} \; exp(-\| \boldsymbol{x}_i,\boldsymbol{x}_k \|^2 \; / \; 2\sigma_i^2)}
	\end{equation*}
	
	\vspace{0.2cm}
	
	in dense sets $ \sigma_i $ should be small, while it should be big when dealing with sparse sets.
\end{frame}

\begin{frame}
	\frametitle{t-Distributed Stochastic Neighbor Embedding}
	
	\vspace{0.3cm}
	
	\begin{itemize}
		\item Second, it defines a similar probability distribution $ Q $ over the points in the low-dimensional map,
			  and it minimizes the \emph{Kullback-Leibler} divergence between the two distributions with respect to
			  the locations of the points in the map
	\end{itemize}
	
	\begin{equation*}
		q_{ij} = \frac{(1 + \| \boldsymbol{y}_i - \boldsymbol{y}_j \|^2)^{-1}}{\sum_{k \neq l} \; (1 + \| \boldsymbol{y}_k - \boldsymbol{y}_l \|^2)^{-1}}
	\end{equation*}
	
	\vspace{0.3cm}
	
	\begin{equation*}
		KL(P||Q) = \sum_{i \neq j} p_{ij} \, log\frac{p{ij}}{q{ij}}
	\end{equation*}
	
	\vspace{0.2cm}
	
	The minimization of the \emph{Kullback-Leibler} divergence is performed using the \emph{gradient descent}.
\end{frame}
